{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "\n",
    "## Exact Gradient Computation\n",
    "\n",
    "Given a function f, sometimes we can compute its exact gradient at any x if f's derivative is easy to compute. For example, let $f(x)=2x^2-3x+\\ln x$, where $x>0$. Please compute the derivative of f and report its gradient at $x=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\\\n",
    "df/dx = 4x -3 +1/x \\\n",
    "when x = 2 \\\n",
    "df/dx|x=2 = 4*2-3+1/2 = 5.5 \\\n",
    "Therefore, the gradient at x = 2 is 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Computation [5 pts]\n",
    "\n",
    "Instead of computing the derivative of a function, we can also estimate the gradient numerically with various methods. These methods are essential, especially when a callable function is not exposed due to privacy reasons, or it is hard to differentiate analytically. \n",
    "\n",
    "To numerically compute the gradient, the simple way is to follow Newton's difference quotient: $f'(x)=\\lim_{h\\to 0}{f(x+h)-f(x)\\over h}$. Another two-point formula is to compute the slope through the points $(x-h,f(x-h))$ and $(x+h,f(x+h))$. Let us reuse the example function $f(x)=2x^2-3x+\\ln x$ and test the precision of these two approaches. Define the function in the next cell, and try to compute its gradient via both methods at $x=2$. Range h value in [0.1,0.01,0.001,0.0001] and report all gradients calculated. Which method is more accurate, and why does it work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def f(x):\n",
    "    return 2*x**2 -3*x + math.log(x)\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h value:  0.1  slope estimated:  5.687901641694317\n",
      "h value:  0.01  slope estimated:  5.518754151103744\n",
      "h value:  0.001  slope estimated:  5.50187504165045\n",
      "h value:  0.0001  slope estimated:  5.5001875004201395\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the first method (Newton's difference quotient)\n",
    "h = [0.1,0.01,0.001,0.0001]\n",
    "x = 2\n",
    "for hvalue in h:\n",
    "    slope = (f(x+hvalue)-f(x))/hvalue\n",
    "    print('h value: ', hvalue, ' slope estimated: ', slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h value:  0.1  slope estimated:  5.500417292784909\n",
      "h value:  0.01  slope estimated:  5.500004166729067\n",
      "h value:  0.001  slope estimated:  5.500000041665842\n",
      "h value:  0.0001  slope estimated:  5.500000000417948\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the second method \n",
    "for hvalue in h:\n",
    "    slope = (f(x+hvalue)-f(x-hvalue))/(hvalue *2)\n",
    "    print('h value: ', hvalue, ' slope estimated: ', slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-point formula is more accurate when computing the gradient. Because two-point formula provides an estimate of the slope of the function over a small interval, rather than relying solely on the function value at a single point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: You may find the gradient more accurate when using a smaller h value. However, this is not always the case. Due to the finite precision of the floating-point, rounding errors always exist and can dominate the computation when the h value is too small. Run the following two cells to observe such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.551115123125783\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-15\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-20\n",
    "print((f(2+eps)-f(2-eps))/2/eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a classification tool that models the probability of an event taking place by having the log odds for the event be a linear combination of one or more independent variables. Specifically, let $\\vec{x}=<x_1,\\dots ,x_m>$ be an m dimensional vector of independent variables (features), and $y$ be the corresponding binary dependent variable (label). The probability of having $y=1$ is modeled as $$P_y={1\\over 1+e^{-(b_0+b_1\\cdot x_1+\\dots +b_m\\cdot x_m)}}={1\\over 1+e^{-(b_0+\\vec{b}_{1:m}\\cdot\\vec{x})}}$$\n",
    "\n",
    "Given a set of data points $<\\vec{x}_k,y_k>$ with $k\\in [1,n]$, how can we fit the model with these data, i.e., how to choose the best $\\vec{b}=b_0,b_1\\cdots,b_m$?\n",
    "\n",
    "One way is to write out the likelihood $$\\prod_{k:y_k=1}P_{y_k}\\prod_{k:y_k=0}(1-P_{y_k})$$ and find $b_0,b_1\\cdots,b_m$ that maximize its logarithm, $$l=\\sum_{k:y_k=1}\\ln(P_{y_k})+\\sum_{k:y_k=0}\\ln(1-P_{y_k})$$\n",
    "\n",
    "In contrast to computing the closed form gradient of a Least-squares loss in a linear model (chapter 5 of MML book), doing the same for logistic regression is not possible. Gradient descent can be used to optimize such function $l$, and we will implement it step-by-step. First, write a function log_likelihood in the next cell that computes the log-likelihood given data points and $\\vec{b}$. [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X,y,b):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    b: one dimension numpy data array of length m+1\n",
    "    \n",
    "    Return the log likelihood.\n",
    "    \"\"\"\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    log_likelihood = 0\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        linear_combination = np.dot(X[i], b)\n",
    "        probability_y = 1.0 / (1 + np.exp(-linear_combination))\n",
    "\n",
    "        if y[i] == 1:\n",
    "            log_likelihood += np.log(probability_y)\n",
    "        else:\n",
    "            log_likelihood += np.log(1 - probability_y)\n",
    "\n",
    "    return log_likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your log_likelihood function with a small example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.031735331771901"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[0.1],[0.5],[1.]])\n",
    "y=np.array([0,0,1])\n",
    "b=np.array([0.,1.])\n",
    "# Your answer should be around -2.03\n",
    "log_likelihood(X,y,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to maximize, the next step is to compute the current gradient for parameter $\\vec{b}$. Use the method with Newton's difference quotient, and set $h=0.0001$. Implement the function compute_gradient in the next cell that returns the gradients of $\\vec{b}$. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,b):\n",
    "# The inputs are the same as the ones of log_likelihood\n",
    "    h = 0.0001\n",
    "    gradient = np.zeros_like(b)\n",
    "    b_copy = b.copy()\n",
    "    original = log_likelihood(X, y, b_copy)\n",
    "    for j in range(len(b)):\n",
    "        b_copy[j] += h\n",
    "        log_likelihood_plus = log_likelihood(X, y, b_copy)\n",
    "        b_copy[j] -= h\n",
    "        gradient[j] = (log_likelihood_plus - original) / h\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.87853115, -0.09479906])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function here, preserve the output\n",
    "compute_gradient(X,y,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to compute the gradients, we can optimize the objective, which is log-likelihood in our case, using gradient descent. It iteratively changes the parameters in a small \"step\" towards the gradient direction, i.e., the direction where the objective increases at the fastest pace. Formally, denote the calculated gradients as $\\Delta (\\vec{b})$, we can update our parameters via $\\vec{b}=\\vec{b}+\\gamma \\cdot \\Delta (\\vec{b})$, where $\\gamma$ is the size of the \"step\". Repeat this process until the objective stop improving or a pre-set max number of iterations is reached. **Note in practice, the value of gradient changes over iterations and can be very large/small, so you should normalize the gradient vector every iteration, i.e., scale it to $\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2$, before using it to compute the new $\\vec{b}$. Therefore, the update rule for parameters becomes $\\vec{b}=\\vec{b}+\\gamma \\cdot {\\Delta (\\vec{b})\\over ||\\Delta (\\vec{b})||_2}$**.\n",
    "\n",
    "Implement the gradient_descent function below. [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, initial_b, step_size, max_iteration):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    initial_b: one dimension numpy data array of length m+1\n",
    "    step_size: scalar, the size of one step update\n",
    "    max_iteration: scalar, the max number of iterations\n",
    "    Return the updated coefficient vector b.\n",
    "    \"\"\"\n",
    "    b = initial_b.copy()\n",
    "    i = 0\n",
    "    lll = float('-inf')\n",
    "    while i < max_iteration:\n",
    "        g = compute_gradient(X,y,b)\n",
    "        norm = np.linalg.norm(g)\n",
    "        newlll = log_likelihood(X,y,b)\n",
    "        if newlll < lll:\n",
    "            return b, i\n",
    "        else:\n",
    "            b = b + step_size * g/norm\n",
    "            i +=1\n",
    "        lll = newlll  \n",
    "\n",
    "    return b, i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with the previous example again. Print for each sample from X, based on your model, the probability of having label=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_b:  [-60.22487755  80.4605066 ]  number of iterations:  1000\n",
      "Probabiliy of having label=1 :  [2.18284757e-23 2.07226370e-09 9.99999998e-01]\n"
     ]
    }
   ],
   "source": [
    "optimized_b, iterations = gradient_descent(X, y, b, 0.1, 1000)\n",
    "\n",
    "# compute and print the probability for each row in X below using optimized_b\n",
    "print('optimized_b: ', optimized_b, ' number of iterations: ', iterations)\n",
    "X_copy = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "linear_combination = np.dot(X_copy, optimized_b)\n",
    "probability_y = 1 / (1 + np.exp(-linear_combination))\n",
    "print('Probabiliy of having label=1 : ', probability_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the implemented logistic regression model to a real dataset. The dataset is a trimmed breast-cancer-Wisconsin dataset from UCI machine learning Repository. Only 100 data points are offered in the training set to make sure the computation can be finished swiftly, no matter how you implement the optimizer. The training dataset is loaded in the next cell, and the vector $\\vec{b}$ is also randomly initialized. \n",
    "\n",
    "Fit three models with the training set using different step size ranging in [0.01,0.05,0.1] and set the max number of iterations as 10000. How do the final log-likelihood value and the number of iterations change with different step sizes? [7 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"breast-cancer-wisconsin.data\",\"r\")\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_train.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_train.append(tmp)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "random_b = np.random.uniform(0,1,size=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final log-likelihood:  -7.4854623637060165\n",
      "number of iterations:  6598\n",
      "trainng size:  0.01\n",
      "final coefficient vector b:  [-1.49480881e+01  9.53163934e-01 -1.41299489e+00  9.33563095e-01\n",
      "  9.97443527e-01  4.71190119e-01  3.42615276e-03  8.70188760e-01\n",
      "  1.05566649e+00  1.44284202e+00]\n",
      "\n",
      "final log-likelihood:  -20.111163866982643\n",
      "number of iterations:  272\n",
      "trainng size:  0.05\n",
      "final coefficient vector b:  [-2.93258165  0.222884    0.24609098  0.16036489  0.16039202  0.04539256\n",
      "  0.19246876 -0.41041347  0.42188665  0.10393277]\n",
      "\n",
      "final log-likelihood:  -46.455825306890965\n",
      "number of iterations:  33\n",
      "trainng size:  0.1\n",
      "final coefficient vector b:  [ 0.21350136 -0.12515667  0.44552376  0.21588382  0.03471867  0.01867804\n",
      "  0.19773108 -0.66589446  0.33953741  0.03622703]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit three models with different step size, report the final log-likelihood, \n",
    "# number of iterations and the final coefficent vector b.\n",
    "training_step = [0.01,0.05,0.1]\n",
    "max_iter = 10000\n",
    "optimized_b = []\n",
    "for step in training_step:\n",
    "    opt_b, iterations = gradient_descent(X_train, y_train, random_b, step, max_iter)\n",
    "    final_likelihood = log_likelihood(X_train,y_train,opt_b)\n",
    "    optimized_b.append(opt_b)\n",
    "    print('final log-likelihood: ', final_likelihood)\n",
    "    print('number of iterations: ', iterations)\n",
    "    print('trainng size: ', step)\n",
    "    print('final coefficient vector b: ', opt_b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the final log-likelihood value and the number of iterations change with different step sizes? [7 pts]\\\n",
    "Based on the test, the smaller the training step is, the larger the final log-likelihood value is. This means that the smaller training step can make the approximzation from gradient descent more likely to be accurate.\\\n",
    "However, the smaller the training step is, the larger number of iterations it uses to get untill no improvement in approximation. The larger number of iterations lead to a longer running time, which is more time-consuming/expensive especially if we have a large training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, load the test dataset, and predict for each sample in the test set what labels it should have using the model obtained. Compare your results with the ground truth labels, and report the accuracy rate. [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_data.txt\",\"r\")\n",
    "X_test = []\n",
    "y_test = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_test.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_test.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_b for use:  [-1.49480881e+01  9.53163934e-01 -1.41299489e+00  9.33563095e-01\n",
      "  9.97443527e-01  4.71190119e-01  3.42615276e-03  8.70188760e-01\n",
      "  1.05566649e+00  1.44284202e+00]\n",
      "Probabiliy of having label=1:  0.0259\n",
      "Probabiliy of having label=1:  0.9938\n",
      "Probabiliy of having label=1:  1.0\n",
      "Probabiliy of having label=1:  1.0\n",
      "Probabiliy of having label=1:  0.9912\n",
      "Probabiliy of having label=1:  1.0\n",
      "Probabiliy of having label=1:  0.0021\n",
      "Probabiliy of having label=1:  1.0\n",
      "Probabiliy of having label=1:  0.0188\n",
      "Probabiliy of having label=1:  0.171\n",
      "Estimation:  [0, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
      "\n",
      "optimized_b for use:  [-2.93258165  0.222884    0.24609098  0.16036489  0.16039202  0.04539256\n",
      "  0.19246876 -0.41041347  0.42188665  0.10393277]\n",
      "Probabiliy of having label=1:  0.1498\n",
      "Probabiliy of having label=1:  0.1805\n",
      "Probabiliy of having label=1:  0.9985\n",
      "Probabiliy of having label=1:  0.8459\n",
      "Probabiliy of having label=1:  0.9961\n",
      "Probabiliy of having label=1:  0.9929\n",
      "Probabiliy of having label=1:  0.2123\n",
      "Probabiliy of having label=1:  0.9734\n",
      "Probabiliy of having label=1:  0.1689\n",
      "Probabiliy of having label=1:  0.9372\n",
      "Estimation:  [0, 0, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "\n",
      "optimized_b for use:  [ 0.21350136 -0.12515667  0.44552376  0.21588382  0.03471867  0.01867804\n",
      "  0.19773108 -0.66589446  0.33953741  0.03622703]\n",
      "Probabiliy of having label=1:  0.3182\n",
      "Probabiliy of having label=1:  0.0576\n",
      "Probabiliy of having label=1:  0.9818\n",
      "Probabiliy of having label=1:  0.7791\n",
      "Probabiliy of having label=1:  0.9962\n",
      "Probabiliy of having label=1:  0.9976\n",
      "Probabiliy of having label=1:  0.6776\n",
      "Probabiliy of having label=1:  0.9365\n",
      "Probabiliy of having label=1:  0.4762\n",
      "Probabiliy of having label=1:  0.9571\n",
      "Estimation:  [0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict based on your models and report the accuracy\n",
    "estimations = []\n",
    "for test in X_test:\n",
    "        test.insert(0, 1)\n",
    "for b in optimized_b:\n",
    "    print('optimized_b for use: ', b)\n",
    "    estimation = []\n",
    "    for test in X_test:\n",
    "        linear_combination = np.dot(test, b)\n",
    "        probability_y = 1 / (1 + np.exp(-linear_combination))\n",
    "        if probability_y < 0.5:\n",
    "            estimation.append(0)\n",
    "        else:\n",
    "            estimation.append(1)\n",
    "        print('Probabiliy of having label=1: ', round(probability_y,4))\n",
    "    print('Estimation: ', estimation)\n",
    "    print()\n",
    "    estimations.append(estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true y:  [0, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "Accuracy Rate:  0.9\n",
      "Accuracy Rate:  0.9\n",
      "Accuracy Rate:  0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('true y: ', y_test)\n",
    "for estimation in estimations:\n",
    "    print('Accuracy Rate: ', accuracy_score(y_test, estimation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
